# bot/jhpiego_bot.py (VERS√ÉO CORRIGIDA)
import os
import re
import logging
import PyPDF2
import docx
import pandas as pd
from collections import Counter

logger = logging.getLogger(__name__)

class JhpiegoBot:
    def __init__(self, upload_dir='uploads'):
        self.UPLOAD_DIR = upload_dir
        self._ensure_upload_dir()
        
        # T√≥picos expandidos para sa√∫de
        self.TOPIC_KEYWORDS = {
            "hiv": ["hiv", "sida", "aids", "v√≠rus", "antiretroviral", "cd4", "carga viral", "arv", "tratamento"],
            "malaria": ["mal√°ria", "mosquito", "plasmodium", "anopheles", "quinino", "febre", "parasita", "sintomas"],
            "tuberculose": ["tb", "tuberculose", "bacilo", "pulmonar", "tosse", "bcg", "mycobacterium", "diagn√≥stico"],
            "gravidez": ["gravidez", "gesta√ß√£o", "parto", "pr√©-natal", "obst√©trica", "neonatal", "cuidados"],
            "vacina√ß√£o": ["vacina", "imuniza√ß√£o", "vacina√ß√£o", "calend√°rio", "dose", "imune", "campanha"],
            "cuidados infantis": ["crian√ßa", "infantil", "pedi√°trico", "neonatal", "rec√©m-nascido", "aleitamento"],
            "nutri√ß√£o": ["nutri√ß√£o", "alimenta√ß√£o", "dieta", "suplemento", "vitamina", "desnutri√ß√£o"],
            "higiene": ["higiene", "sanit√°rio", "limpeza", "lavagem", "saneamento", "preven√ß√£o"]
        }
        
        logger.info("‚úÖ Jhpiego Bot inicializado (vers√£o melhorada)")

    def _ensure_upload_dir(self):
        """Garante que o diret√≥rio de uploads existe"""
        if not os.path.exists(self.UPLOAD_DIR):
            os.makedirs(self.UPLOAD_DIR)
            logger.info(f"üìÅ Diret√≥rio '{self.UPLOAD_DIR}' criado")

    # --- Fun√ß√µes para ler arquivos (MANTIDAS) ---
    def read_txt(self, file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logger.error(f"‚ùå Erro ao ler TXT {file_path}: {e}")
            return ""

    def read_pdf(self, file_path):
        try:
            text = ""
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    text += page.extract_text() + "\n"
            return text
        except Exception as e:
            logger.error(f"‚ùå Erro ao ler PDF {file_path}: {e}")
            return ""

    def read_docx(self, file_path):
        try:
            doc = docx.Document(file_path)
            return "\n".join([p.text for p in doc.paragraphs])
        except Exception as e:
            logger.error(f"‚ùå Erro ao ler DOCX {file_path}: {e}")
            return ""

    def read_excel(self, file_path):
        try:
            df = pd.read_excel(file_path)
            return df.to_string(index=False)
        except Exception as e:
            logger.error(f"‚ùå Erro ao ler Excel {file_path}: {e}")
            return ""

    def clean_terms(self, text):
        """Limpa e tokeniza texto de forma mais eficaz"""
        stopwords = {
            "o", "a", "os", "as", "de", "do", "da", "que", "√©", "e", "para", "em", "um", "uma",
            "com", "por", "se", "na", "no", "nas", "nos", "uma", "um", "em", "por", "para", "com",
            "n√£o", "sim", "como", "mas", "ou", "porque", "porqu√™", "quando", "onde", "qual"
        }
        # Melhor regex para capturar palavras
        words = re.findall(r'\b[a-z√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√ß]{3,}\b', text.lower())
        return [w for w in words if w not in stopwords]

    def detect_topic(self, question):
        """Detecta t√≥pico com scoring melhorado"""
        q = question.lower()
        topic_scores = {}
        
        for topic, keywords in self.TOPIC_KEYWORDS.items():
            score = 0
            for kw in keywords:
                # B√¥nus maior se a palavra-chave for exata
                if f" {kw} " in f" {q} ":
                    score += 3
                elif kw in q:
                    score += 2
            
            if score > 0:
                topic_scores[topic] = score
        
        if topic_scores:
            best_topic = max(topic_scores.items(), key=lambda x: x[1])
            logger.info(f"üéØ T√≥pico detectado: {best_topic[0]} (score: {best_topic[1]})")
            return best_topic[0]
        
        return None

    def calculate_advanced_similarity(self, question, text):
        """Calcula similaridade mais inteligente"""
        question_terms = self.clean_terms(question)
        text_terms = self.clean_terms(text)
        
        if not question_terms:
            return 0.0
        
        # Contar ocorr√™ncias
        question_counter = Counter(question_terms)
        text_counter = Counter(text_terms)
        
        # Calcular similaridade ponderada
        total_score = 0
        for word, q_count in question_counter.items():
            if word in text_counter:
                # Score baseado na frequ√™ncia e import√¢ncia
                word_score = min(q_count, text_counter[word])
                
                # Palavras mais longas s√£o mais importantes
                length_bonus = min(len(word) / 10, 0.5)
                
                total_score += word_score + length_bonus
        
        # Normalizar pelo n√∫mero de palavras √∫nicas na pergunta
        normalized_score = total_score / len(question_terms)
        
        # B√¥nus para textos que cont√™m palavras da pergunta no in√≠cio
        first_200_chars = text[:200].lower()
        bonus = 0
        for word in question_terms:
            if word in first_200_chars:
                bonus += 0.1
        
        final_score = min(normalized_score + bonus, 1.0)
        return final_score

    def semantic_search(self, question):
        """Busca sem√¢ntica MELHORADA com ranking"""
        try:
            documents = []
            
            # Primeiro: coletar todos os documentos
            for filename in os.listdir(self.UPLOAD_DIR):
                path = os.path.join(self.UPLOAD_DIR, filename)
                if os.path.isfile(path) and filename.endswith(('.txt', '.pdf', '.docx', '.xlsx')):
                    content = ""
                    
                    if filename.endswith('.txt'):
                        content = self.read_txt(path)
                    elif filename.endswith('.pdf'):
                        content = self.read_pdf(path)
                    elif filename.endswith('.docx'):
                        content = self.read_docx(path)
                    elif filename.endswith('.xlsx'):
                        content = self.read_excel(path)
                    
                    if content.strip():
                        documents.append({
                            'filename': filename,
                            'content': content,
                            'score': 0
                        })
            
            if not documents:
                logger.warning("‚ùå Nenhum documento encontrado para busca")
                return None, None
            
            # Segundo: calcular score para CADA documento
            scored_documents = []
            for doc in documents:
                score = self.calculate_advanced_similarity(question, doc['content'])
                doc['score'] = score
                scored_documents.append(doc)
            
            # Terceiro: ORDENAR por score (maior primeiro)
            scored_documents.sort(key=lambda x: x['score'], reverse=True)
            
            # Log detalhado dos scores
            logger.info("üìä Ranking de documentos:")
            for i, doc in enumerate(scored_documents[:3]):  # Top 3
                logger.info(f"   {i+1}. {doc['filename']}: {doc['score']:.3f}")
            
            # Quarto: retornar o MELHOR documento (se tiver score suficiente)
            best_doc = scored_documents[0]
            
            if best_doc['score'] > 0.15:  # Threshold ajustado
                logger.info(f"üéØ Selecionado: {best_doc['filename']} (score: {best_doc['score']:.3f})")
                return best_doc['content'], best_doc['filename']
            else:
                logger.warning(f"‚ö†Ô∏è Melhor score muito baixo: {best_doc['score']:.3f}")
                return None, None
            
        except Exception as e:
            logger.error(f"‚ùå Erro no search sem√¢ntico: {e}")
            return None, None

    def extract_relevant_part(self, full_text, question, topic):
        """Extrai a parte MAIS RELEVANTE baseada na pergunta"""
        # Dividir em par√°grafos significativos
        paragraphs = []
        for p in re.split(r'\n\s*\n', full_text):
            clean_p = p.strip()
            if len(clean_p) > 30:  # Par√°grafos muito curtos s√£o ignorados
                paragraphs.append(clean_p)
        
        if not paragraphs:
            return full_text[:400] + "..." if len(full_text) > 400 else full_text
        
        # Score cada par√°grafo baseado na pergunta
        scored_paragraphs = []
        question_terms = self.clean_terms(question)
        topic_keywords = self.TOPIC_KEYWORDS.get(topic, [])
        
        for i, paragraph in enumerate(paragraphs):
            score = 0
            
            # 1. Score por palavras da PERGUNTA (mais importante)
            for word in question_terms:
                if word in paragraph.lower():
                    score += 2  # B√¥nus maior para palavras da pergunta
            
            # 2. Score por keywords do T√ìPICO
            for kw in topic_keywords:
                if kw in paragraph.lower():
                    score += 1.5
            
            # 3. B√¥nus para par√°grafos que respondem perguntas diretas
            question_lower = question.lower()
            if any(q_word in question_lower for q_word in ['como', 'quando', 'onde', 'qual', 'quais']):
                if any(a_word in paragraph.lower() for a_word in ['deve', 'dever', 'precisa', 'necessita', 'recomenda']):
                    score += 1
            
            # 4. B√¥nus para par√°grafos de tamanho ideal
            if 80 <= len(paragraph) <= 600:
                score += 0.5
            
            scored_paragraphs.append((score, paragraph, i))
        
        # Ordenar por score e pegar os MELHORES
        scored_paragraphs.sort(reverse=True)
        
        # Selecionar os 2 melhores par√°grafos com score > 0
        top_paragraphs = []
        for score, paragraph, idx in scored_paragraphs:
            if score > 0 and len(top_paragraphs) < 2:
                top_paragraphs.append(paragraph)
            elif len(top_paragraphs) >= 2:
                break
        
        # Se n√£o encontrou par√°grafos relevantes, usar estrat√©gia fallback
        if not top_paragraphs:
            # Tentar encontrar par√°grafos que contenham palavras da pergunta
            for paragraph in paragraphs:
                if any(term in paragraph.lower() for term in question_terms[:3]):  # 3 primeiras palavras
                    top_paragraphs.append(paragraph)
                    if len(top_paragraphs) >= 2:
                        break
            
            # Fallback final: primeiros par√°grafos
            if not top_paragraphs:
                top_paragraphs = paragraphs[:2]
        
        # Juntar os par√°grafos selecionados
        result = "\n\n".join(top_paragraphs)
        
        # Garantir que a resposta n√£o seja muito longa
        if len(result) > 800:
            result = result[:800] + "..."
        
        return result

    def process_query(self, question):
        """Processa a pergunta e retorna resposta MELHORADA"""
        question = question.strip()
        
        if not question:
            return {
                "response": "Por favor, fa√ßa uma pergunta.",
                "topic": None,
                "source": None
            }
        
        logger.info(f"üîç Processando pergunta: '{question}'")
        
        # Detectar t√≥pico
        topic = self.detect_topic(question)
        
        if not topic:
            return {
                "response": "N√£o consegui identificar o tema espec√≠fico da sua pergunta. Pode mencionar HIV, Malaria, Tuberculose, Gravidez, ou outro tema de sa√∫de?",
                "topic": None,
                "source": None
            }
        
        # Busca sem√¢ntica MELHORADA
        relevant_doc, doc_name = self.semantic_search(question)
        
        if not relevant_doc:
            return {
                "response": f"N√£o encontrei informa√ß√µes espec√≠ficas sobre '{question}' nos documentos dispon√≠veis.",
                "topic": topic,
                "source": None
            }
        
        # Extrair parte MAIS RELEVANTE
        best_response = self.extract_relevant_part(relevant_doc, question, topic)
        
        logger.info(f"‚úÖ Resposta gerada - T√≥pico: {topic} - Documento: {doc_name}")
        
        return {
            "response": best_response,
            "topic": topic,
            "source": doc_name
        }

    def generate_faq(self, limit=12):
        """Gera perguntas frequentes automaticamente a partir dos documentos"""
        documents_text = ""

        # 1) Ler todos os documentos
        for filename in os.listdir(self.UPLOAD_DIR):
            path = os.path.join(self.UPLOAD_DIR, filename)
            if os.path.isfile(path) and filename.endswith(('.txt', '.pdf', '.docx', '.xlsx')):
                try:
                    if filename.endswith('.txt'):
                        documents_text += self.read_txt(path) + "\n"
                    elif filename.endswith('.pdf'):
                        documents_text += self.read_pdf(path) + "\n"
                    elif filename.endswith('.docx'):
                        documents_text += self.read_docx(path) + "\n"
                    elif filename.endswith('.xlsx'):
                        documents_text += self.read_excel(path) + "\n"
                except:
                    continue
        
        if not documents_text.strip():
            return []

        # 2) Dividir texto em frases
        sentences = re.split(r'(?<=[.!?])\s+', documents_text)

        candidate_faq = []
        question_prefixes = ("como", "quando", "onde", "qual", "quais", "o que", "por que", "quem", "deve", "precisa")
        
        # 3) Selecionar frases relevantes
        for s in sentences:
            s_clean = s.strip()
            s_low = s_clean.lower()

            # Tem perfil de pergunta ou orienta√ß√£o
            if any(p in s_low[:50] for p in question_prefixes):
                if 30 < len(s_clean) < 160:  # evitar frases curtas ou longas demais
                    candidate_faq.append(s_clean)

        # 4) Evitar duplicados e limitar quantidade
        faq_unique = list(dict.fromkeys(candidate_faq))[:limit]

        return faq_unique


# Inst√¢ncia global do bot
jhpiego_bot = JhpiegoBot()